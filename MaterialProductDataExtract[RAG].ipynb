{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPVg0WfIL6xCScW0db3yhP6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JanMeow/2025Hack/blob/main/MaterialProductDataExtract%5BRAG%5D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "yDs72BNF6e-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RAG for material product data verification\n",
        "This notebook serves to extract information commonly seen in construction project such as pdf, images or plans\n",
        "We will use different ML appraoches to extract the information we need.\n",
        "And we see how do we create insights out of these data ! ✌"
      ],
      "metadata": {
        "id": "f2ckgvqt6fyM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, Lets install some dependecy !"
      ],
      "metadata": {
        "id": "ecwOoRXu_RTE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b72xqUdK6anI",
        "outputId": "a70fe784-6bb7-43dd-fcac-22414a7ac9b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.1.0)\n",
            "Collecting pdfminer.six\n",
            "  Downloading pdfminer.six-20240706-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six) (2.22)\n",
            "Downloading pdfminer.six-20240706-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pdfminer.six\n",
            "Successfully installed pdfminer.six-20240706\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (3.4 kB)\n",
            "Downloading pymupdf-1.25.3-cp39-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (20.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m80.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf\n",
            "Successfully installed pymupdf-1.25.3\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m78.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.8.2)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pillow\n",
        "!pip install pdfminer.six\n",
        "!pip install --upgrade pymupdf\n",
        "!pip install requests\n",
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets get an exmple product declaration\n",
        "Here we are using  [The offical CreaBeton Sample](https://pepadocs.com/de/openWindow/Documents/byCode/694928caefd60034a8ce50c30ab540da/asMainPage/)\n",
        "\n",
        "Here we are using the pdf of\n",
        "Technische Wegleitung Betonsteinbeläge\n",
        "for RAG it has 54 pages, so we can also test if it would exceed the context window."
      ],
      "metadata": {
        "id": "w9NxApAhBKhK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get an example product declarat\n",
        "import requests\n",
        "from pathlib import Path\n",
        "r = requests.get(\"https://img.socialcraft.me/cache/fileUgWYMwmDT88KPnEFlnTN7vspZVcf4W1EmPkXMy8gKYlKwAnifwwSFhd6pS0L/wegleitung-betonsteinbelaege.pdf\")\n",
        "\n",
        "\n",
        "pdf_folder = Path(\"data\")\n",
        "pdf_folder.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "pdf_path = pdf_folder/\"sample.pdf\"\n",
        "\n",
        "with open(pdf_path, \"wb\") as f:\n",
        "    f.write(r.content)\n",
        "\n",
        "\n",
        "pdf_path = \"sample.pdf\"\n"
      ],
      "metadata": {
        "id": "B-tPaaWRAIJ2"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pdfminer.high_level import extract_text, extract_pages\n",
        "\n",
        "texts = extract_text(pdf_path)"
      ],
      "metadata": {
        "id": "mNUZjawqDEIF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(texts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3A0IYjsEQp9",
        "outputId": "03d52efe-e17d-4444-8534-1f8ac631dfd8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "22075"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, there are usually  A LOT OF words for a product declaration.\n",
        "This is difficult for the model because it exceeds its context window\n",
        "There are a few ways to acheive it.\n"
      ],
      "metadata": {
        "id": "wlTIamiBEa6h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Fine-Tuning\n",
        "2. Chuck RAG\n",
        "3. Sliding Window Attention\n",
        "4. Summarization and Pre-processing"
      ],
      "metadata": {
        "id": "GJm4hiAnFODv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "since Fine-tuning would need to train part of the model or train added layers and also product infomration changes all the time. This is not an efficient apporach.\n",
        "In this notebook, we will experiment with a few techniques\n",
        "- Passing the text to a summarization model before RAG (4.Sumnmarization and Pre-processing)\n",
        "- Perform Chuck RAG"
      ],
      "metadata": {
        "id": "FAGch3ESFgj-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the later cells I could also perform fine tuning.\n",
        "But it will be focused more on the regulations or LCA data since these are not changed as frequently."
      ],
      "metadata": {
        "id": "haZiP1WGN-Gl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Formatting tool for Construction Product"
      ],
      "metadata": {
        "id": "KNoMI4vW11N3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\\Note that currently I summarise it session by session, one could also do it page by page. But I figure by session it is better.\n",
        "\n",
        "In future practice, we might want to specify certain font sizes for the user manual or add tag so we can extract information from the pdf.\n",
        "The other option is to train one more small linear model, in which we read text from many documents and label the font size in relation to the title and content so the model can decide but this is for later."
      ],
      "metadata": {
        "id": "xJS3ai9YPbxj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting and formating the text for LLM  to summarise the text and generate tags as a basis to create a vector database"
      ],
      "metadata": {
        "id": "9vxB1bM63c5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fitz\n",
        "from transformers import pipeline\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "from pydantic import BaseModel\n",
        "\n",
        "\n",
        "pdf = fitz.open(pdf_path)\n",
        "\n",
        "\n",
        "# Get your own MotherFucking key\n",
        "api_key = userdata.get(\"OpenAiKey\")\n",
        "\n",
        "\n",
        "# For test purpose, in this particular document, text sizes are 9.5 and title size are either 10 or 14\n",
        "\n",
        "test_page = pdf[4]\n",
        "text_dict = test_page.get_text(\"dict\")\n",
        "\n",
        "def extract_text(pdf, title_font_size):\n",
        "  page_content = []\n",
        "\n",
        "  for block in text_dict[\"blocks\"]:\n",
        "    if \"lines\" in block:\n",
        "      for line in block[\"lines\"]:\n",
        "        if \"spans\" in line:\n",
        "          for span in line[\"spans\"]:\n",
        "            # print(span[\"size\"], span[\"text\"])\n",
        "            if span[\"size\"] >= title_font_size:\n",
        "              if len (page_content) >0:\n",
        "                # Because sometimes the title are broken into few lines\n",
        "                if page_content[-1][\"text\"] == \"\":\n",
        "                  page_content[-1][\"title\"] += span[\"text\"]\n",
        "                  continue\n",
        "              dict_obj = {\n",
        "                \"title\":span[\"text\"],\n",
        "                \"text\":\"\"\n",
        "              }\n",
        "              page_content.append(dict_obj)\n",
        "            else:\n",
        "              if len(page_content) >0:\n",
        "                # .replace because german documents have this weird thing\n",
        "                cleaned = span[\"text\"].replace(\"\\xad\", \"\")\n",
        "                page_content[-1][\"text\"] += cleaned\n",
        "  return page_content\n",
        "\n",
        "def summarise_OpenAi(text, max_word, api_key):\n",
        "\n",
        "  class Summary(BaseModel):\n",
        "    summary: list[str]\n",
        "\n",
        "  client = OpenAI(\n",
        "      api_key=api_key,\n",
        "  )\n",
        "\n",
        "  response = client.beta.chat.completions.parse(\n",
        "      model=\"gpt-4o-mini\",\n",
        "       messages = [\n",
        "      {\"role\": \"user\", \"content\": f\"Summarise {text} and generate {max_word} keywords from the text in the orignal language\"},\n",
        "      ],\n",
        "      response_format= Summary\n",
        "  )\n",
        "\n",
        "  outout = response.choices[0].message.parsed\n",
        "  return outout\n",
        "\n",
        "\n",
        "\n",
        "# ==============================================================================================================\n",
        "page_content = extract_text(test_page, 12)\n",
        "\n",
        "for content in page_content:\n",
        "  text = content[\"text\"]\n",
        "  word_counts = len(text.split())\n",
        "\n",
        "  # I want to dynamically summraise the word counts, meaning that if the passage is longer, it could have a slightly longer summary.\n",
        "  response = summarise_OpenAi(text, round(word_counts * 0.1), api_key)\n",
        "  content[\"summary\"] = response.summary\n",
        "\n",
        "# =============================================================================================================="
      ],
      "metadata": {
        "id": "PbtCNJ2jRCKM"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for content in page_content:\n",
        "  print(content[\"title\"])\n",
        "  print(content[\"summary\"])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy0wqKC3cZVH",
        "outputId": "4f7bc853-5058-43d9-bc1e-02ab4572feba"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Innenwand, tragend\n",
            "['Brandschutz bezieht sich auf Maßnahmen und Praktiken, die der Verhütung und Bekämpfung von Bränden dienen. Ziel ist es, Menschenleben zu schützen, Sachwerte zu bewahren und Umweltschäden zu minimieren. Es umfasst sowohl bauliche Maßnahmen, wie feuerbeständige Materialien, als auch organisatorische Aspekte, wie das Erstellen von Notfallplänen und regelmäßige Schulungen. Die Einhaltung von Vorschriften und Normen ist entscheidend für die Effektivität des Brandschutzes.']\n",
            " \n",
            "['TragwerkR 60 Brandabschnitt mit Aufbau aus zweilagiger Gipsfaserplatte (2x 15 mm), einem 160 mm breiten Ständer und 160 mm Mineralwolle RF1 (SP > 1000° C), abgedeckt mit erneut zweilagiger Gipsfaserplatte (2x 15 mm).', 'Optimal für Feuerwiderstandsklassen, um den Brandschutz in Gebäuden zu gewährleisten.']\n",
            "Decke (DE-01)\n",
            "['Brandschutz ist ein System von Maßnahmen und Vorschriften, die darauf abzielen, Brände zu verhüten und die Sicherheit von Personen und Sachwerten zu gewährleisten.', 'Es umfasst technische, organisatorische und bauliche Maßnahmen.', 'Zu den technischen Maßnahmen gehören Rauchmelder und Sprinkleranlagen, während zu den organisatorischen Maßnahmen Fluchtpläne und regelmäßige Brandschutzübungen zählen.', 'Des Weiteren sind auch die Verwendung feuerbeständiger Materialien und die Einhaltung von Sicherheitsstandards im Bauwesen wichtige Aspekte des Brandschutzes.', 'Gesetzliche Vorgaben, wie die Landesbauordnungen, definieren die Mindestanforderungen für den Brandschutz in öffentlichen und privaten Gebäuden.']\n",
            " \n",
            "['TragwerkR', 'Brandabschnitt EI 60', 'Bodenbelag 15 mm', 'Trittschalldämmung 30 mm']\n",
            " \n",
            "['Wiederkehr Industriestrasse 9, Postfach, Switzerland', 'Contact: +41 (0)62 765 15 35, info@holzbauing.ch', 'Project Number: CH-5712 Beinwil am See', 'Website: www.holzbauing.ch', 'Plan Number: Makiol Wiederkehr AG, Neubau.']\n",
            " \n",
            "['Wohnen ist ein zentraler Aspekt des menschlichen Lebens, der sowohl physische als auch soziale Dimensionen umfasst. Es bezieht sich auf den Wohnraum, in dem Menschen leben, sowie auf die Art und Weise, wie sie ihren Raum gestalten und nutzen. Der Begriff umfasst nicht nur Häuser und Wohnungen, sondern auch das Umfeld, das Wohngemeinschaften und Nachbarschaftsbeziehungen beeinflusst. Die Gestaltung des Wohnraums kann die Lebensqualität, den Komfort und das Wohlbefinden der Bewohner stark beeinflussen.']\n",
            " \n",
            "[]\n",
            " \n",
            "['AlterWOL-2312700rg / jtVorprojektLandstrasse in 5426 Lengnau ist ein Bauprojekt mit einer letzten Revision zum Stand -110. Es beinhaltet tragende Geschossübergänge (IW).', 'Das Hauptaugenmerk liegt auf der Umsetzung von tragenden Strukturen innerhalb des Projekts.']\n",
            "RG\n",
            "[]\n",
            " \n",
            "['1Geschossübergang IW tragendRG refers to structural regulations regarding load-bearing floor transitions in buildings, ensuring that these transitions maintain structural integrity and safety. It outlines specifications for the materials and designs used at these junctures within architectural projects.']\n",
            "Detail 1          \n",
            "['Wohnung A ist eine moderne und helle Wohnimmobilie in einer ruhigen Gegend. Sie verfügt über zwei Schlafzimmer, ein geräumiges Wohnzimmer, eine voll ausgestattete Küche und ein Badezimmer. Die Wohnung hat große Fenster, die viel Tageslicht hereinlassen, sowie einen Balkon mit Blick auf den Garten. Sie ist ideal für kleine Familien oder Paare und befindet sich in der Nähe von Einkaufsmöglichkeiten und öffentlichen Verkehrsmitteln. Zudem sind Parkplatzmöglichkeiten vorhanden und die Nachbarn sind freundlich.']\n",
            "ENTWURFVariante 2\n",
            "['AABBSchnitt: A method or style of cutting, possibly in textiles or design.', 'A-ASchnitt: Refers to another specific cut or design technique, emphasizing a distinct approach.', 'A-A: May denote a classification or simplification of the previous terms, indicating a linking concept or aspect.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have used OpenAi for the summrization Task.\n",
        "(Although originally I would like to use Deepseek cause its much cheaper lollll)\n",
        "We could either turn the text back to a queryable instead of saving the entire text, but since this doesnt affect the pipeline but more so on the memories side I decided to leave it aside.\n"
      ],
      "metadata": {
        "id": "-0xw1Xu_R4Y_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a vector DB and a Graph DB"
      ],
      "metadata": {
        "id": "XeA4gPhjeuxN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chroma - the open-source embedding database using OpenAi Embedding !!!"
      ],
      "metadata": {
        "id": "OG9oUzj2gT-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naturally, if you use Chroma DB to embed the data but unfortunately most environmental documents in switzerland are in german so we will need to use the OpenAi embedding"
      ],
      "metadata": {
        "id": "NYqBBXAnmGUv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First lets create some embeddings from our text.\n",
        "from openai import OpenAI\n",
        "from functools import reduce\n",
        "\n",
        "# dimension is set to 384 as by default OpenAi embeddings are of dinension 1536 but ChromaDB takes dimension 384\n",
        "def get_embedding(text, model=\"text-embedding-ada-002\", dimensions = 384):\n",
        "\n",
        "  client = OpenAI(\n",
        "      api_key=api_key,\n",
        "  )\n",
        "\n",
        "  response = client.embeddings.create(\n",
        "      input=text,\n",
        "      model=\"text-embedding-3-small\",\n",
        "      dimensions=dimensions\n",
        "  )\n",
        "\n",
        "  return response.data[0].embedding\n",
        "\n",
        "for content in page_content:\n",
        "  summary = \" \".join(content[\"summary\"])\n",
        "  response = get_embedding(summary, model = \"text-embedding-ada-002\")\n",
        "  content[\"embedding\"] = response"
      ],
      "metadata": {
        "id": "xwIZv66nnXCF"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(page_content[0][\"embedding\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D24dIHgeo3BQ",
        "outputId": "983ff391-f0ee-4a8b-c6be-798d7f1c1595"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "384"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# OK ! Finally time to create the vector DB\n",
        "\n",
        "!pip install chromadb --quiet\n",
        "\n",
        "import chromadb\n",
        "# setup Chroma in-memory, for easy prototyping. Can add persistence easily!\n",
        "chroma_client = chromadb.Client()\n",
        "\n",
        "# Create collection. get_collection, get_or_create_collection, delete_collection also available!\n",
        "collection = chroma_client.get_or_create_collection(\"all-my-documents\")\n",
        "\n",
        "# # Add docs to the collection. Can also update and delete. Row-based API coming soon!\n",
        "\n",
        "for i, content in enumerate(page_content):\n",
        "  document = content[\"text\"]\n",
        "  embedding = content[\"embedding\"]\n",
        "  title = content[\"title\"]\n",
        "\n",
        "  collection.add(\n",
        "      documents=document, # we handle tokenization, embedding, and indexing automatically. You can skip that and add your own embeddings as well\n",
        "      metadatas={\"title\": title,\n",
        "                  \"source\": \"page4, lineXXX PlaceHolder\"}, # filter on these!\n",
        "      ids=f\"id_{i}\", # unique for each doc\n",
        "      embeddings=embedding\n",
        "  )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GIMZzOgV742",
        "outputId": "f1883669-3a9b-451d-e1a8-645c31990988"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m52.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m76.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m79.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.9/55.9 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m177.4/177.4 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.7/118.7 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m89.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets query our vectorDB ! Meow !"
      ],
      "metadata": {
        "id": "2z9_VWlbz6LF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pset0 = {'BaseQuantities': {'NetSurfaceArea': 30.3922,\n",
        "  'NetVolume': 5.2284,\n",
        "  'id': 7386},\n",
        " 'Cadwork3dProperties': {'Group': 'Fenster',\n",
        "  'SubGroup': 'Fenster',\n",
        "  'BTA TYP': 'Fenster 16.1',\n",
        "  'id': 7390},\n",
        " 'BIMWood_Common': {'Local coordinate system': {'id': 7393,\n",
        "   'type': 'IfcComplexProperty',\n",
        "   'UsageName': 'Local coordinate system',\n",
        "   'properties': {'Location': {'id': 7394,\n",
        "     'type': 'IfcComplexProperty',\n",
        "     'UsageName': 'Location',\n",
        "     'properties': {'X': 224.9999741, 'Y': 6510.0000975, 'Z': 7214.9501083}},\n",
        "    'Axis': {'id': 7396,\n",
        "     'type': 'IfcComplexProperty',\n",
        "     'UsageName': 'Axis',\n",
        "     'properties': {'X': 0.0, 'Y': -1.0, 'Z': 0.0}},\n",
        "    'RefDirection': {'id': 7397,\n",
        "     'type': 'IfcComplexProperty',\n",
        "     'UsageName': 'Reference Direction',\n",
        "     'properties': {'X': -1.0, 'Y': 0.0, 'Z': 0.0}}}},\n",
        "  'id': 7392},\n",
        " 'BIMWood_Production': {'ProductionNumber': '0',\n",
        "  'Package': '',\n",
        "  'Layer': 0,\n",
        "  'id': 7399}}"
      ],
      "metadata": {
        "id": "lDH_3IPUnBvc"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pset1 = {'BaseQuantities': {'NetSurfaceArea': 107.1924,\n",
        "  'NetVolume': 2.6723,\n",
        "  'id': 888},\n",
        " 'Cadwork3dProperties': {'Group': 'Fassade EG',\n",
        "  'SubGroup': 'AW 1.OG',\n",
        "  'BTA TYP': 'IS 15.1',\n",
        "  'id': 892},\n",
        " 'BIMWood_Common': {'Local coordinate system': {'id': 898,\n",
        "   'type': 'IfcComplexProperty',\n",
        "   'UsageName': 'Local coordinate system',\n",
        "   'properties': {'Location': {'id': 899,\n",
        "     'type': 'IfcComplexProperty',\n",
        "     'UsageName': 'Location',\n",
        "     'properties': {'X': 100.9998339, 'Y': 6524.5001187, 'Z': 5615.2774545}},\n",
        "    'Axis': {'id': 903,\n",
        "     'type': 'IfcComplexProperty',\n",
        "     'UsageName': 'Axis',\n",
        "     'properties': {'X': 1.0, 'Y': 0.0, 'Z': 0.0}},\n",
        "    'RefDirection': {'id': 904,\n",
        "     'type': 'IfcComplexProperty',\n",
        "     'UsageName': 'Reference Direction',\n",
        "     'properties': {'X': 0.0, 'Y': -1.0, 'Z': 0.0}}}},\n",
        "  'id': 897},\n",
        " 'BIMWood_Production': {'ProductionNumber': '0',\n",
        "  'Package': '',\n",
        "  'Layer': 0,\n",
        "  'id': 906}}"
      ],
      "metadata": {
        "id": "Hc5dwRv7tDBh"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Query/search 2 most similar results. You can also .get by id\n",
        "# We need to change the query function from chroma DB abit because we used OpenAi for embedding\n",
        "def query(query_texts, n_results):\n",
        "  query_embeddings = get_embedding(query_texts, model=\"text-embedding-ada-002\")\n",
        "  results = collection.query(\n",
        "      query_embeddings= query_embeddings,\n",
        "      n_results=n_results,\n",
        "      # where={\"metadata_field\": \"is_equal_to_this\"}, # optional filter\n",
        "      # where_document={\"$contains\":\"search_string\"}  # optional filter\n",
        "  )\n",
        "  return results\n",
        "\n",
        "\n",
        "query(f\"what is the best conncetion type for the entity0 with {pset0} and entity1 {pset1}  \", 1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Fip27kPz3WZ",
        "outputId": "415e195f-42e7-4dd1-b676-c18b1388781a"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'ids': [['id_9']],\n",
              " 'embeddings': None,\n",
              " 'documents': [['1Geschossübergang IW tragendRG']],\n",
              " 'uris': None,\n",
              " 'data': None,\n",
              " 'metadatas': [[{'source': 'page4, lineXXX PlaceHolder', 'title': ' '}]],\n",
              " 'distances': [[1.4482554197311401]],\n",
              " 'included': [<IncludeEnum.distances: 'distances'>,\n",
              "  <IncludeEnum.documents: 'documents'>,\n",
              "  <IncludeEnum.metadatas: 'metadatas'>]}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Graph DB Set up"
      ],
      "metadata": {
        "id": "k44zFjloWn2a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Jdd3AzFYWtD8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cCFUj8uDWnSe"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Objective for today\n",
        "Graph Database\n",
        "finish the summarization part\n",
        "maybe revise a bit on SKlearn"
      ],
      "metadata": {
        "id": "iUCYR2fckteG"
      }
    }
  ]
}